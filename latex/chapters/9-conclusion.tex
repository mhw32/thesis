\section{Summary of Contributions}

The motivating theme of this dissertation was to build and use technical machinery to do probabilistic inference with neural networks efficiently and scalably for real world applications. There is a rich literature on generative model and their applications to modern ML systems \cite{murphy2022probabilistic,goodman2018probabilistic}. Generative models, and their probabilistic relatives, provide a framework for humans to bias model learning and prediction with their domain expertise,  which can be especially powerful in applications with limited computational and annotation resources. While generative models have been popularized in computer vision and natural language \cite{radford2021learning,ramesh2021zero,brown2020language}, there remains a question of their broad applicability to science and engineering where data quality is low: challenges of missing data, multi-modal data, and strong resource constraints are common. In this dissertation, we take steps towards generalizing variational inference to these complex settings across three parts. Next, we summarize the contributions within each of these parts.

In \textbf{Part 1}, we setup the problem for deep inference in Chapter~\ref{chapter:background}, introducing the concepts of variational inference as well as prominent frameworks for deep generative modeling of high-dimensional data. Then, we discussed algorithmic extensions to deep inference, focusing on two directions: scalability and efficiency. For the first direction, we discussed an extension to inference with multimodal high dimensional data, such as images and natural language, in Chapter~\ref{chapter:mvae}. In this setting, algorithms developed must be robust to missing modalities post-training. We introduced a multimodal extension to the evidence lower bound objective that be optimized with standard gradient descent to compute a joint approximate posterior over all modalities as well all conditional posteriors. For the second direction, we discussed a more efficient sampling procedure for stochastic variational inference in Chapter~\ref{alg:antithetic}. Here, we presented the usage of antithetic samples to learning in deep generative models, where we can achieve the performance of importance weighting with many samples while posing less of a computational burden. This technique may be especially interesting in contexts where data is very complex and multiple samples are needed to reduce variance of computing gradients.

In \textbf{Part 2}, we studied an alternative approach to efficiency and scalability of probabilistic inference: by doubly amortizing inference through neural networks trained to perform supervised regression, we presented a new inference framework in Chapter~\ref{chapter:metavae} that is shared across a family of generative models. For a new generative model (within the same meta-distribution), deep inference can be done zero-shot, reducing the computational cost significantly. In Chapter~\ref{chapter:foundation}, we generalized the idea of meta-amortized inference beyond variational autoencoders to probabilistc programs through masked language modeling and transformer networks. By treating inference as a language modeling task, we constructed a foundation posterior that can used for both zero-shot inference, and finetuned through optimization for high quality inference when computation is permissible.

Finally, in \textbf{Part 3}, we considered two real-world applications of deep inference for computational education. The first challenge was to estimate ability from sequential student responses, such as to a survey or a course examination. In Chapter~\ref{chapter:vibo}, we recast item response theory (IRT), a popular framework for modeling student ability jointly with characteristics of the survey or exam question, as a deep inference problem. By doing so, we introduced several extensions to the IRT model by parameterizing the response function as a deep generative model. Lastly, in Chatper~\ref{chapter:ggnap}, we study a second problem of providing educational feedback to open-ended student solutions. Unlike IRT where student solutions are correct or incorrect, open-ended solutions are highly unstructued, such as programming code. Since annotated data is too expensive to collect, we introduced a generative model from which we can sample synthetic feedback jointly with student solutions. Through deep inference, we showed that this approach matches the quality of feedback from human instructors in several domains.

\section{Future Directions}

Within education and even beyond, probabilistic models together with inference algorithms can make a large impact in practical applications. Despite the wealth of data on the internet that we commonly study in machine learning, most applications in science and engineering cannot reap the benefits of ``big data''. In education, student classrooms are limited size; collecting thousands of labeled data points would take years not considering the cost. Deep generative models pose a promising solution to this data scarcity. But in order to have a high quality generative models, we require high quality inference. This dissertation presented advancements for probabilistic inference using new techniques from variational methods and deep learning. This contributions of this dissertation suggest new opportunities and challenges for future work. We discuss a few of these below.\newline

\noindent\textbf{Large Scale Generative Models.} Modern generative models for computer vision and natural language \cite{radford2021learning,ramesh2021zero,brown2020language} have departed from the explicit latent variable model to self-supervised objectives \cite{chen2020improved,chen2020simple,chen2021exploring,grill2020bootstrap}. Although the exact nature of the relationship between contrastive learning and generative models remains to be understood, the two intuitively appear closely related \cite{zimmermann2021contrastive,liu2021self}. If this is the case, what is the implicit generative model and inference procedure? Can we leverage what we know about latent variable models and deep inference to improve and build new self-supervised models?  \newline

\noindent\textbf{Automatic Inference for Probabilistic Programs.} Two challenges to the widespread adoption of probabilistic programming are new syntax (to specify distributions, random variables, and observations) as well as the technical understanding to perform inference (which often requires users to design inference protocols). Recent frameworks like Pyro \cite{bingham2019pyro} or Edward \cite{tran2017deep} have made leaps and bounds towards alleviating this overhead. Building in this direction, the foundation posterior from Chapter~\ref{chapter:foundation} takes another step towards an automated inference engine. Further work can investigate extensions of it to non-continuous random variables, more complex program structures, and bounding its generalization capabilities.  \newline

\noindent\textbf{A Generative Tutor for Scalable Education.} Estimating student ability and providing feedback to student solutions are two components of a broader challenge of scalable and equitable education. Generative models and deep inference may have additional roles in other components, such as curriculum design, problem generation \cite{srivastava2021question}, or instructor feedback. At the current moment, the many components of education all fall under the role of the instructor. Given the ability to truly ``understand'' students at scale, generative models could provide the framework for an automated teaching assistant, enabling teachers to spend less time on low-level responsibilities like grading and more effort on teaching and content creation.\newline  

In summary, we believe advancements in generative models and inference will continue to have a profound impact in the machine learning community and more generally, the scientific community. In particular, education and machine learning remain largely disjointed communities. But as student data becomes more readily accessible, machine learning has the potential for positive impact in improving student learning, and making better teaching experience \textit{scalably}.
