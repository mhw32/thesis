\epigraph{Inference is always an invasion of the unknown, a leap from the known.}{John Dewey}

Over the last decade, we have seen a Cambrian explosion of deep learning algorithms that together leverage large amounts of annotated data with vast computational resources to solve prediction tasks. These methods have been so successful that deep models are the de-facto approach for prediction in a variety of applications such as image classification \cite{krizhevsky2012imagenet} and machine translation \cite{bahdanau2014neural}. While impressive, this new paradigm for machine learning, and more broadly for artificial intelligence (AI) stands in stark contrast to the constraints of many practical applications in science and engineering. In many contexts, data is rare and quality is low. In education, class sizes are hundreds of students at most, limiting datasets to be magnitudes of order less than what deep learning is typically accustomed to. More over, student data is often multimodal and unstructured, riddled with missing entries and anomalous outliers. This distinction raises the question: how do we leverage deep learning in these real world applications?

In a contrasting lens, humans are able to learn very complex functions about the real world without copious amounts of annotated data. Somehow, humans as intelligent systems are able to make \textit{inferences} about their observations and decisions both efficiently and accurately. Ideally, we would like to be able to replicate this same inference procedure in the technical machinery we develop. Towards this goal, the field of probabilistic models has a rich history of designing inference procedures to transform prior beliefs to posterior knowledge. More recently, deep learning tools have been mixed with more traditional probabilistic modeling to build more powerful inference engines. Can these new tools tackle the challenge of inference in real world applications where data is sparse and noisy? 

\section{Approach}

As humans, we navigate the world and make observations through various input channels. Together, these observations provide information to make beliefs about phenomena we did not or can not observe. For instance, when observing that the lawns is wet, one might infer that it rained (or perhaps one might infer that the sprinkler went off?). The goal of an inference algorithm, for humans and probabilistic models alike, is to infer the most likely beliefs for these hidden (or latent) phenomena from observations (or data). 

% The goal of an inference model is to automate the estimation of latent phenomena from data. 
This, in general, is a very challenging problem. Computational algorithms for inference are still far from their human equivalents. For one, they can be difficult to scale to the size and richness of real world data. The focus of this dissertation will be designing new inference algorithms, motivated by these real world challenges of sparsity and scale. To this end, we organize the dissertation around three key organizational themes, which we discuss next.

\subsection{Efficient and Scalable Inference}

Statistical inference algorithms are often studied on low-dimensional real-valued datasets. Even for modern deep inference algorithms, the use cases are limited to a single modality (e.g. images). In most real world contexts, data is both multimodal and high dimensional. For instance, in a captioned video, we are given audio, language, and visual data, each of which comes from a complex data distribution. In order to infer actions of objects in the video, an inference algorithm would need to capture a joint posterior over all modalities. In this richly multimodal setting, as the number of modalities increase, we cannot expect to have all modalities present. For instance, not all videos have closed captioning. A robust inference algorithm must be resilient to missing data. We develop a generalization of variational inference for modeling multimodal data distributions \cite{wu2018multimodal}, and leverage a product factorization of unimodal posteriors to handle missing modalities. Together, these contributions improve the performance of the variational autoencoder \cite{wu2018multimodal} in machine translation, image generation, and more.

Beyond multimodality, deep inference for high dimensional complex data can be computationally expensive. For data distributions that are difficult to capture, it is common for practitioners to use multiple samples to reduce variance in gradients estimates. However, this can greatly increase the computational cost. We provide a new technique \cite{wu2019differentiable} for sample-efficient learning of generative models by leveraging differentiable antithetic sampling techniques \cite{wu2019differentiable,ren2019adaptive}, resulting in better quality inference using multiple samples at cheaper cost.

\subsection{Meta-Amortized Inference}

For humans, inference is compositional: we are able to extract patterns and use that to perform similar future inferences quickly. In probabilistic models however, inference is much less dynamic. Prior to \cite{gershman2014amortized,kingma2013auto,rezende2014stochastic}, probabilistic inference was done for a single assignment to observed variables. If we received a second inference ``query'' for a novel assignment to the \textit{same} observed variables, we would need to re-do the computation entirely. For variational inference, this meant re-solving a potentially computationally intensive optimization problem from scratch. To alleviate this burden, \cite{gershman2014amortized} introduced the concept of \textit{amortization} where an inference model is trained to quickly perform inference for a wide range of assignments. But can we do more? 

We propose to doubly amortize our inference models to quickly perform inference for both a wide range of assignments but also a wide range of probabilistic models. Through a \textit{meta-amortized} inference algorithm \cite{wu2020meta}, we further improve upon singly-amortized inference models in efficiency. For a novel generative model and novel assignments to the observed variables, a meta-amortized inference model performs inference with minimal computation. 

First, we apply the framework to variational autoencoders \cite{wu2020meta}, deriving a meta- evidence lower bound that can be optimized to train a meta-amortized inference model. We show such a generative model is competitive on a variety of computer vision tasks. Second, we generalize the framework beyond variational autoencoders to the wider class of probabilistic programs. We show that inference can be framed as a language modeling task by masking and unmasking assignments of variables in the raw program string. Leveraging recent innovations with transformer networks \cite{vaswani2017attention} and masked language modeling \cite{devlin2018bert,liu2019roberta}, we show that meta-amortized inference is scalably in more complex probabilistic settings. By training a single inference model across a collection of probabilistic programs, we construct a \textit{foundation posterior}: a single inference model that can be finetuned on any novel probabilistic program for efficient inference. We observe the foundation posterior to be competitive with ADVI \cite{kucukelbir2015automatic,kucukelbir2016automatic,ranganath2014black} and MCMC \cite{murphy2022probabilistic,carpenter2017stan} baselines. 

\subsection{Real World Applications}

A culmination of the challenges of real world inference, education as a domain is limited in data with difficult long-tailed distributions \cite{wu2018zero,malik2019generative} that poses difficulties for deep learning. However, there is a real opportunity for machine learning to play an impactful role in education, where there are many opportunities to empower instructors to better understand student learning. 

Generative models and deep inference can play a key role in real time analysis and response to students interacting with educational content. Broadly, there are two distinct questions: how well does a student understand the material? Second, what misconceptions might a student have? To study the former, we ground this problem in examination theory through item response models \cite{wu2020variational,wu2021modeling}. Traditional approaches to inferring student ability, such as numerical integration, MCMC, or expectation-maximization, either make strong assumptions or face computational hurdles. We showcase that deep inference can be combined with item response theory to create richer models of student cognition that are high dimension and nonlinear. Inference in this context can remain efficient despite the cognitive model growing in complexity, leading to faster and more accurate assessment. We observe encouraging results across a variety of large-scale real-world examinations, such as multiple worldwide scholastic exam for high school students.

Measuring student ability misses out on a crucial component of student learning, which is the granularity on which concepts a student is struggling with. In many ways, this granularity is what an educator truly cares about. We study the challenge of inferring student misconceptions on open-ended student work, spanning block-based programming assignments to short-form essay responses. To circumvent data scarcity, we design a generative model to produce synthetic student solutions and synthetic misconceptions, which we leverage to learn a deep inference model \cite{wu2018zero,malik2019generative}. Across multiple data modalities (programming code, rendered graphics, short response), we observe near human-level performance on identifying misconceptions, with a 50\% improvement over baselines. In a real classroom, we ran an experiment where we used the inference model to augment human teaching assistants, yielding doubled grading accuracy while halving grading time.

\section{Dissertation Overview}

This dissertation is organized into three parts. In Chapter~\ref{chapter:background}, we provide the necessary background in generative models and inference. Concepts presented here will be useful review for all three parts.\newline

\noindent\textbf{Part 1} investigates the efficiency and scalability of deep inference algorithms.
\begin{itemize}
    \item In Chapter~\ref{chapter:mvae}, we present a generative model and inference procedure for multi-modal data, where an unknown subset of modalities are missing in test time. This chapter is based on \cite{wu2018multimodal} and includes results from \cite{wu2018multimodal}.
    \item In Chapter~\ref{chapter:antivae}, we present a sampling method using antithetics for more efficient inference when learning generative models. This chapter is based on \cite{wu2019differentiable}.
\end{itemize}

\noindent\textbf{Part 2} investigates meta-amortized inference algorithms to cheaply do inference for a novel generative model. 
\begin{itemize}
    \item In Chapter~\ref{chapter:metavae}, we present a generalization of the variational autoencoder to perform meta-amortized inference over a distribution of data distributions. This chapter is based on \cite{wu2020meta}. 
    \item In Chapter~\ref{chapter:foundation}, we present a novel framework for meta-inference on probabilistic programs, where we present inference as a masked language modeling task. This chapter is based on \cite{wu2022foundation}.
\end{itemize}

\noindent\textbf{Part 3} investigates the use of deep inference for real world applications in education. 
\begin{itemize}
    \item In Chapter~\ref{chapter:vibo}, we present a deep inference framework for item response theory suitable to estimate student ability in large scale examinations. We use it to model a variety of language learning and examination datasets, from DuoLingo to the Programme for International Student Assessment to the Trends in International Mathematics and Science Study. The chapter is based on \cite{wu2021modeling} and builds on our earlier work in \cite{wu2020variational}.
    \item In Chapter~\ref{chapter:ggnap}, we present a deep inference algorithm for estimating educational feedback for open-ended student solutions to programming assignments. This chapter is based on \cite{malik2019generative} and builds on our prior work in \cite{wu2019zero}.
\end{itemize}

\noindent We summarize the major contributions in this dissertation and exciting open directions for future research in Chapter~\ref{chapter:conclusion}.
