Despite the growth of data size, many applications for which we would like to apply learning algorithms are limited by data quantity and quality. Generative models propose a framework to naturally combine prior beliefs with real world data. Core to the generative approach is the challenge of probabilistic inference, or estimating latent variables given observations. This challenge has led to a rich field of research spanning many classes of statistical techniques. More recently, deep learning methods have been re-tooled to solve inference queries, aptly named \textit{deep inference}. In my dissertion I will explore algorithmic extensions to deep inference in response to real world challenges of sparsity and efficiency. I will present case studies of practical applications where deep inference achieves considerable improvements upon prior work.

This dissertation is centered around three parts. We  present the background for generative models and deep inference with an emphasis on modern variational methods. The first part will present new algorithms for generalizing inference to be robust to different notions of sparsity, such as multimodal data, missing data, or computational constraints. Second, we study \textit{meta}-amortized inference, or ``inferring how to infer''. A doubly-amortized inference algorithm would be cheaply able to solve inference queries for a novel generative model. We will show a new algorithm to re-purpose masked language modeling to do just this.

Third, we present three real-world applications of deep inference for: (a) estimating student abilities under item response theory and related psychometric models, (b) unsupervised feedback for student solutions to programming questions, and (c) compression of high resolution satellite imagery. Together, these contributions showcase the richness and utility of deep inference in the real world.